{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio sobre la tienda on-line de:\n",
    "\n",
    "## [AMANTIS](https://www.amantis.net/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué este estudio?\n",
    "\n",
    "Primero, este es un trabajo de análisis de datos y de predicción sobre una tienda on-line. Creo que las personas que veamos esta documentación somos adultas y lo suficientemente maduras para asumir que el sexo es parte de nuestra vida.\n",
    "\n",
    "Por este motivo, el hacer un estudio sobre una tienda on-line erótica es lo mismo que realizarlo sobre una tienda de ropa, muebles, un supermercado,...\n",
    "\n",
    "Segundo, yo soy usuario registrado de esta tienda on-line. No solo realizo compras para mí o para mi pareja sino también para otras personas. \n",
    "Es por este motivo el que recibo periódicamente correos de ofertas de esta tienda on-line y es aquí en donde entra mi motivación personal.\n",
    "\n",
    "Los correos que envían a sus usuarios están basados en ofertas que lanzan (estas ofertas están basadas en rebajas) y son muy arbitrarias. Suelo recibir correos de ofertas de determinados productos de los que NO he realizado una compra del mismo o similares. \n",
    "\n",
    "Por este motivo, considero interesante poder realizar algún tipo de estudio sobre los productos que tengan en la tienda y poder dar una mejor experiencia al comprador."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones para el proyecto.\n",
    "\n",
    "Este proyecto está estructurado en 4 partes:\n",
    "\n",
    "- Obtención y tratamiento de datos de la página web, a partir de *Webscrapping* y su manipulación para poder tratarlos a través de las librerías de *pandas* y *numpy* y su almacenamiento en una *Base de datos Relacional*, dada la posibilidad de limitar los campos a realizar y la atomización de los datos.\n",
    "   - A su vez se realizará algún tipo de visionado de los datos obtenidos para ver diversos aspectos a tener en cuenta.\n",
    "- Generación de nuevas variables, *featuring engineering*, para poder dar una mejor visión de los productos que ofrecen en la tienda.\n",
    "    - A través de la generación de estas nuevas variables, pretendemos dar un visionario más completo sobre la relación de las diversas variables entre sí y los usuarios.\n",
    "    - Estas variables pueden ser obtenidas a través de *pandas*, *RegEx* y *NLP*.\n",
    "- Análisis de la información obtenida y generada para ver la relación existente entre usuarios, productos, fechas...\n",
    "   - Esta información será aplicada a partir de diversas queries de la base de datos a utilizar y con las librerías de *pandas*, *matplotlib* y *seaborn*.\n",
    "- Generación de modelados de los datos obtenidos para establecer gustos de los usuarios y dar sugerencia de otros productos que tengan etiquetas similares.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Obtención y tratamiento de los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parte del proyecto ya está presentada como un proyecto individual, por lo cual para seguir su desarrollo les remito al susodicho proyecto ([repositorio de Webscrapping](https://github.com/75Engel/Webscrapping_tienda_online)).\n",
    "\n",
    "Aqui lo que vamos a presentar como detalle es el codigo final que se ha utilizado para la obtención de los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de la librería para Webscrapping *Beautiful Soap* procederemos a extraer esta información y en determinados casos a tratarla para poder trabajar con ella.\n",
    "\n",
    "Para ello cargaremos las librerías necesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estas serían las librerias importadas:\n",
    "\n",
    "~~~~\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estas serían las variables necesarias:\n",
    "\n",
    "~~~\n",
    "url_principal=\"https://www.amantis.net/\"                        \n",
    "url_secundaria=url_principal+'productos-amantis/'\n",
    "\n",
    "date=str(datetime.today().strftime('%y%m%d'))\n",
    "folder_ingest=r'.\\Data'\n",
    "\n",
    "ext=r'.csv'\n",
    "scrape='_scrape'\n",
    "file_product=r'\\productos'\n",
    "file_user=r'\\usuarios'\n",
    "file_comment=r'\\comentarios'\n",
    "file_price=r'\\precios'\n",
    "file_tag=r'\\tags'\n",
    "file_ingest_product=file_product+scrape\n",
    "file_ingest_comment=file_comment+scrape\n",
    "\n",
    "product_ingest=folder_ingest+file_ingest_product+'_'+date+ext\n",
    "comment_ingest=folder_ingest+file_ingest_comment+'_'+date+ext\n",
    "product_engineer=folder_ingest+file_product+'_'+date+ext\n",
    "tag_engineer=folder_ingest+file_tag+'_'+date+ext\n",
    "price_engineer=folder_ingest+file_price+'_'+date+ext\n",
    "comment_engineer=folder_ingest+file_comment+'_'+date+ext\n",
    "user_engineer=folder_ingest+file_user+'_'+date+ext\n",
    "\n",
    "pages= np.arange(1,25)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Este sería el código para la obtención de la información:\n",
    "\n",
    "~~~~\n",
    "'''Listas a generar con la información de los productos'''\n",
    "\n",
    "lista_URLs = []\n",
    "name=[]\n",
    "regular_prices=[]\n",
    "new_price=[]\n",
    "info=[]\n",
    "id=[]\n",
    "comentarios=[]\n",
    "\n",
    "'''Generamos 2 diccionarios con los datos importantes para ingresar en una BBDD'''\n",
    "\n",
    "diccionario_datos_productos={\"ID\":id,\"NAME\":name,\"INFO\":info,\"LISTA_URL\":lista_URLs,\"REGULAR_PRICE\":regular_prices,\"DISCOUNT_PRICE\":new_price}\n",
    "diccionario_comentarios_productos={\"ID\":id,\"COMENTARIOS\":comentarios}\n",
    "\n",
    "'''Listas para generar la información de los comentarios'''\n",
    "id_comment=[]\n",
    "comments=[]\n",
    "date=[]\n",
    "ratio=[]\n",
    "users=[]\n",
    "comment=[]\n",
    "\n",
    "print(\"Empezando a recoger datos de las paginas\")\n",
    "for page in pages:\n",
    "    \n",
    "    if page == 1:\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[9:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[9:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "print(\"Terminando de recoger los datos de los links de las paginas\\nEmpezando a generar las listas de los productos\")\n",
    "\n",
    "for i in range(len(lista_URLs)):\n",
    "    id.append(i)\n",
    "\n",
    "    \n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True)\n",
    "        name.append(nombre)\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:]\n",
    "    documentation = ''.join(information)\n",
    "    info.append(documentation)\n",
    "\n",
    "\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "\n",
    "    \n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "        date_comments_product.append(dates_text)\n",
    "        # date_object = datetime.strptime(date_comments_product)\n",
    "\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = len(heart_rating)\n",
    "        rating.append(num_hearts)\n",
    "\n",
    "    datos = list(zip(date_comments_product,rating, user_comments_product,comments_product ))\n",
    "    comentarios.append(datos)\n",
    "\n",
    "for i, regular_price in enumerate(regular_prices):\n",
    "    if regular_price is None:\n",
    "        regular_prices[i] = new_price[i]\n",
    "\n",
    "print(\"Realizando la ingenieria de los datos\\nEliminando duplicados de productos\")\n",
    "productos=pd.DataFrame(diccionario_datos_productos)\n",
    "noduplicated_product = productos.drop_duplicates(subset='NAME', keep='first')\n",
    "removed_id = productos[productos.duplicated(subset='NAME', keep='first')]['ID']\n",
    "\n",
    "print(\"Tratando los comentarios\")\n",
    "\n",
    "comentarios_productos=pd.DataFrame(diccionario_comentarios_productos)\n",
    "comentarios=pd.DataFrame()\n",
    "diccionario={\"id\":id_comment,\"comments\":comments}\n",
    "\n",
    "for id_product,n_comments in enumerate (comentarios_productos['COMENTARIOS']):\n",
    "    for i in n_comments:\n",
    "        id_comment.append(id_product)\n",
    "        comments.append(i)\n",
    "\n",
    "\n",
    "for j in range(len(diccionario['comments'])):\n",
    "    date.append(diccionario['comments'][j][0])\n",
    "    ratio.append(diccionario['comments'][j][1])\n",
    "    users.append(diccionario['comments'][j][2])\n",
    "    comment.append(diccionario['comments'][j][3])\n",
    "\n",
    "\n",
    "comentarios['ID']=pd.Series(id_comment)\n",
    "comentarios['DATE']=pd.Series(date)\n",
    "comentarios['RATIO']=pd.Series(ratio)\n",
    "comentarios['USERS']=pd.Series(users)\n",
    "comentarios['COMMENT']=pd.Series(comment)\n",
    "\n",
    "noduplicated_comments = comentarios[~comentarios['ID'].isin(productos[productos['ID'].isin(removed_id)]['ID'])]\n",
    "\n",
    "h=input(\"Quieres salvar los datos?\").upper()\n",
    "if h==\"SI\":\n",
    "    noduplicated_product.to_csv(product_ingest,header=True,index=False)           \n",
    "    noduplicated_comments.to_csv(comment_ingest,header=True,index=True)           \n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Ingeniería de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiero hacer notar que parte de este punto se mostro en el anterior proyecto, pero en este repositorio aparece todo el proceso de generación de datos que hemos realizado para obtener nuevas variables que son importantes para analizar las características de los productos, las tendencias de los usuarios y de este modo poder realizar un análisis en profundidad de la información obtenida y el modelado que queremos plantear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los dataframes que se obtienen son los siguientes:\n",
    "\n",
    "*Dataframe productos:*\n",
    "\n",
    "![Dataframe productos](./Resources/image/dataframe_productos.JPG)\n",
    "\n",
    "\n",
    "*Dataframe comentarios:*\n",
    "\n",
    "![Dataframe comentarios](./Resources/image/dataframe_comentarios.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es por ello que en el **dataframe de productos** vamos a generar diversos dataframes con la información más sintetizada posible separando los precios del resto.\n",
    "\n",
    "Mientras que el nombre, la descripción y las características son valores que no variarán en el tiempo y, por lo tanto, pueden permanecer en un **dataframe de productos**, no pasa lo mismo con los precios pueden fluctuar cada vez que se realiza una extracción y por lo tanto hay que dejarlos aparte con una fecha de referencia de cuando se han tomado, creando un **dataframe de precios**.\n",
    "\n",
    "Además crearemos un dataframe completamente nuevo a partir de la columna descripción usando *keywords* existentes en esta columna para asignar *tags* a cada producto. Este dataframe lo llamaremos **dataframe de tags**.\n",
    "\n",
    "Realmente no es necesario crear este dataframe aparte del dataframe de productos, pero lo hemos realizado así para que sea más visible la información resultante del mismo.\n",
    "\n",
    "En el **dataframe de comentarios**, vamos a realizar una atomización de los valores de los usuarios para que el tratamiento de la información sea más manejable.\n",
    "\n",
    "Es por ello que crearemos un **dataframe de usuarios**, más o menos arbitrario dada la imposibilidad de asignar correctamente los usuarios, y un **dataframe de comentarios**, donde haremos una pequeña manipulación del campo *date* para poder realizar posteriormente los análisis precisos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1- Creando dataframe de usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a discriminar en función de si un nombre de usuario a escrito un comentario sobre un producto, si vuelve a aparecer este nombre se genere un nombre nuevo, que será el mismo con un dígito para distinguirlos.\n",
    "\n",
    "Es cierto que esta forma de presentar los datos a través de la página web aumenta el anonimato de las personas que lo compran, ya que lo habitual es que se repitan nombres sin coincidir con un mismo usuario (¿cuántos José hay en España?¿y en el mundo?)\n",
    "\n",
    "No es posible hacer un seguimiento de qué productos compran las personas de este modo, pero vamos a realizar un \"simulacro\" de cómo se debería de realizar.\n",
    "\n",
    "El modo correcto sería usando id de usuarios, lo que solo es posible por parte de la empresa o si se utilizarán nicks para que aparecieran los usuarios que realizan los comentarios por su nick.\n",
    "\n",
    "El código a continuación muestra como se realiza la sustitución \"in situ\" en el dataframe.\n",
    "~~~\n",
    "nombre_count = {}\n",
    "count = {}\n",
    "\n",
    "for i, row in df_comments.iterrows():\n",
    "    id = row['ID']\n",
    "    nombre = row['USERS']\n",
    "    \n",
    "    if id not in count:\n",
    "        count[id] = {}\n",
    "        \n",
    "    if nombre in count[id]:\n",
    "        count[id][nombre] += 1\n",
    "        nuevo_nombre = f\"{nombre}_{count[id][nombre]}\"\n",
    "        df_comments.loc[i, 'USERS'] = nuevo_nombre\n",
    "    else:\n",
    "        count[id][nombre] = 1\n",
    "~~~\n",
    "\n",
    "Posteriormente creamos una diccionario a partir de una lista creada con los valores únicos del campo *USERS* y con los índices de los usuario de esta lista.\n",
    "\n",
    "Con este diccionario agregamos el campo *ID_USERS* con un mapeo en el *dataframe de comentarios* y, posteriormente creamos el *dataframe de usuarios*.\n",
    "\n",
    "~~~\n",
    "'''mapeo de usuarios'''\n",
    "lista_users=df_comments['USERS'].unique()                  \n",
    "mivalor = [ x for x in range(len(lista_users))]             \n",
    "lista_users=list(lista_users)                                \n",
    "lista_users_code = {k: v for k, v in zip(lista_users, mivalor)}   \n",
    "print(lista_users_code)\n",
    "df_comments['ID_USERS']= df_comments['USERS'].map(lista_users_code)\n",
    "\n",
    "'''creación del dataframe de usuarios'''\n",
    "USERS=pd.DataFrame()\n",
    "USERS['ID_USERS']=df_comments['ID_USERS']\n",
    "USERS['USERS']=df_comments['USERS']\n",
    "USERS.drop_duplicates(subset='ID_USERS', keep='first',inplace=True)\n",
    "\n",
    "'''reemplazamos el campo USERS por ID_USERS''' \n",
    "col = df_comments.pop('ID_USERS')\n",
    "df_comments.drop(columns=['USERS'],inplace=True)\n",
    "df_comments.insert(loc= 4 , column= 'ID_USERS', value= col)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2- Creando dataframe de comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya, una vez hemos reemplazado el campo USERS por ID_USERS, nos queda modificar la información que existe en el campo *DATE* para adaptarla a un formato de fecha que podamos trabajar.\n",
    "\n",
    "Recordemos que la información que aparece en este campo tiene esta formulación: miércoles 25 enero, 2023.\n",
    "\n",
    "Y queremos que tenga esta formulación: 2023-01-25.\n",
    "\n",
    "Para ello usaremos el método string para obtener las posiciones 1, 2 y 3 y reemplazar la posición 2 con un mapeado por el número de mes correspondiente.\n",
    "\n",
    "~~~\n",
    "'''Este es el diccionario con el cual realizaremos el mapeado''' \n",
    "dm_mapping={\n",
    "    'enero':1, \n",
    "    'febrero':2, \n",
    "    'marzo':3, \n",
    "    'abril':4, \n",
    "    'mayo':5,\n",
    "    'junio':6, \n",
    "    'julio':7,\n",
    "    'agosto':8, \n",
    "    'septiembre':9, \n",
    "    'octubre':10, \n",
    "    'noviembre':11, \n",
    "    'diciembre':12,\n",
    "} \n",
    "\n",
    "'''Con esta porción de código realizaremos la extracción y el mapeado correspondiente'''\n",
    "df_comments['DAY']=df_comments['DATE'].str.split(' ').str.get(1).astype('Int64')\n",
    "df_comments['MONTH']=df_comments['DATE'].str.split(' ').str.get(2).str.split(',').str.get(0)\n",
    "df_comments['YEAR']=df_comments['DATE'].str.split(' ').str.get(-1).astype('Int64')\n",
    "df_comments['MONTH']=df_comments['MONTH'].map(dm_mapping)\n",
    "\n",
    "'''Aqui regeneramos el campo DATE con la información obtenida'''\n",
    "df_comments['DATE'] = pd.to_datetime(df_comments.iloc[:,-3:])\n",
    "df_comments=df_comments.iloc[:,:-3]\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya hemos generados los dataframes relacionados con los comentarios, vamos a proceder a mostrar como generamos los dataframes relacionados con los productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3- Creando dataframe de precios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello generamos un nuevo campo, que denominados *FECHA* que hace referencia a la fecha en la cual se hace este proceso de ingeniería, con un formato de yymmdd y generamos un nuevo dataframe con los campos *ID_PRODUCT*, *REGULAR_PRICE* y *DISCOUNT_PRICE*. \n",
    "\n",
    "En este proceso lo primero que hacemos es reestructurar el dataframe, ya que ID está separado del resto de los campos que necesitamos y luego recortamos el dataframe.\n",
    "\n",
    "~~~\n",
    "'''Generamos el valor de DATE que vamos a incorporar en el dataframe, así como creamos las variables que hacen referencia a los campos que vamos a dejar en el dataframe''' \n",
    "date=str(datetime.datetime.today().strftime('%y%m%d'))\n",
    "col_1 = dataframe.pop('REGULAR_PRICE')\n",
    "col_2=dataframe.pop('DISCOUNT_PRICE')\n",
    "col_3=dataframe['ID']\n",
    "\n",
    "'''Colocamos nuevamente las variables en las posiciones que queremos dejar'''\n",
    "dataframe.insert(loc= 1 , column= 'ID_PRODUCT', value= col_3)\n",
    "dataframe.insert(loc= 2 , column= 'REGULAR_PRICE', value= col_1)\n",
    "dataframe.insert(loc= 3 , column= 'DISCOUNT_PRICE', value= col_2)\n",
    "df_prices=dataframe.iloc[:,:4]\n",
    "\n",
    "'''Añadimos el campo FECHA'''\n",
    "df_prices['FECHA']=date\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los 2 siguientes puntos vamos a centrarnos en los campos *NAME* e *INFO* que tienen gran información de texto referente al producto (principalmente el segundo campo).\n",
    "\n",
    "Del campo *NAME* nos interesa reducir el volumen de información que existe, por lo cual lo separaremos en 2 campos.\n",
    "\n",
    "El campo *INFO* también lo vamos a separar en 2 campos, del que usaremos principalmente el primer campo que crearemos, aunque el segundo campo sí tiene información que podría ser evaluable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4- Creando dataframe de productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí mostramos como creamos los campos *PRODUCT* y *SLOGAN* a partir del campo *NAME*, para crear los campos *CHARACTERISTICS* y *DESCRIPTION* haremos un procedimiento similar.\n",
    "\n",
    "El campo *NAME* tiene diversas peculiaridades que hemos intentado subsanar con la primera línea de código, ya que *PRODUCT* y *SLOGAN* se encuentran separados por un guión, pero este guión también aparece dentro del texto que queremos dejar en estos campos por lo cual nos vemos obligados a realizar una pequeña sustitución antes de realizar el split.\n",
    "\n",
    "~~~\n",
    "dataframe['NAME'] = dataframe['NAME'].str.replace(r'-(?=\\w)', '_')\n",
    "dataframe[['PRODUCT', 'SLOGAN']] = dataframe['NAME'].str.split('[,-.]', 1, expand=True)\n",
    "dataframe['PRODUCT'] = dataframe['PRODUCT'].str.strip()\n",
    "dataframe['SLOGAN'] = dataframe['SLOGAN'].str.strip()\n",
    "~~~\n",
    "\n",
    "En el campo *INFO* también existe algún caracter que hay que sustituir, y lo trabajamos de un modo similar. En este caso, es la existencia de salto de línea (\\r).\n",
    "\n",
    "Una vez realizada estas transformadas, recolocamos los campos que vamos a dejar en el dataframe de productos y eliminamos el resto.\n",
    "\n",
    "~~~\n",
    "'''Definimos las variables con los nombres de los campos'''\n",
    "col_1 = dataframe.pop('PRODUCT')\n",
    "col_2=dataframe.pop('SLOGAN')\n",
    "col_3=dataframe.pop('DESCRIPTION')\n",
    "col_4=dataframe.pop('CHARACTERISTICS')\n",
    "\n",
    "'''Eliminamos los campos primigenios con los que hemos creado los nuevos campos'''\n",
    "dataframe.drop(columns=['NAME'],inplace=True)\n",
    "dataframe.drop(columns=['INFO'],inplace=True)\n",
    "\n",
    "'''Posicionamos los nuevos campos y reducimos el dataframe'''\n",
    "dataframe.insert(loc= 1 , column= 'PRODUCT', value= col_1)\n",
    "dataframe.insert(loc= 2 , column= 'SLOGAN', value= col_2)\n",
    "dataframe.insert(loc= 3 , column= 'DESCRIPTION', value= col_3)\n",
    "dataframe.insert(loc= 4 , column= 'CHARACTERISTICS', value= col_4)\n",
    "df_product=dataframe.iloc[:,:6]\n",
    "~~~\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5- Creando dataframe de tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la parte más importante del proceso de ingeniería de datos que tiene el proyecto ya que vamos a generar un dataframe nuevo a partir del campo *DESCRIPTION*, dejando también los campos *ID* y *PRODUCT* para poder relacionarlo con el dataframe de producto.\n",
    "\n",
    "Básicamente vamos a crear un *diccionario*, cuya clave será el nombre del nuevo campo creado y cuyo valor será una lista con keywords que evaluaremos con **SpaCy** para asignar un True/False en el caso que aparezca alguna de las keywords.\n",
    "\n",
    "Este es un punto crítico y de continua evaluación, ya que hay que confirmar periódicamente que los productos se etiquetan correctamente.\n",
    "\n",
    "El diccionario que vamos a utilizar es:\n",
    "\n",
    "{**'amenities'**: ['lubricante', 'limpiador', 'preservativo', 'condon'],\n",
    "\n",
    " **'anal'**: ['anal'],\n",
    "\n",
    " **'BDSM'**: ['latex', 'bdsm', 'arnes', 'strap', 'cera', 'ligadura', 'cuerda', 'cuero', 'sumision', 'dominacion', 'latigo', 'watenberg', 'posicionador', 'mordaza'],\n",
    "\n",
    " **'femenino'**: ['mujer', 'femenino', 'vaginal', 'clitoris', 'dildo'],\n",
    "\n",
    " **'masculino'**: ['hombre', 'masculino'],\n",
    "\n",
    " **'juguetes'**: ['dildo', 'plug', 'vibrador', 'masturbador', 'cabezal', 'estimulador', 'plugs', 'bolas chinas', 'funda extensora', 'bomba de vacio'],\n",
    "\n",
    " **'lenceria'**: ['babydoll', 'body', 'camiseta', 'corse', 'corset', 'diadema', 'encaje', 'falda', 'joyeria', 'lenceria', 'lencero', 'malla', 'media', 'panties', 'pantis', 'ropa', 'sujetador', 'vestido'],\n",
    " \n",
    " **'muebles'**: ['chaise', 'columpio', 'moqueta', 'silla', 'sillon', 'sillones']}\n",
    "\n",
    "Este diccionario está creado a partir de ficheros .pickle donde están guardadas cada lista y cuyo nombre es el valor de la clave. Se crea a partir de la función que existe en la carpeta [src\\Utils](Utils/functions.py).\n",
    "\n",
    "A partir de las funciones: **aplicar_funcion_a_columna** y **extraer_lemas** se generan las nuevas variables.\n",
    "\n",
    "~~~\n",
    "def extraer_lemas(text):\n",
    "    '''\n",
    "    Función que recibe un texto como entrada y devuelve una lista de los lemas de las palabras que se encuentran en el texto. \n",
    "    \n",
    "    La lista de lemas resultante solo contiene las palabras que son alfabéticas, es decir, que no contienen caracteres numéricos o especiales.\n",
    "\n",
    "    - Inputs:\n",
    "        - text (str):           Texto del cual se quieren extraer los lemas.\n",
    "\n",
    "    - Outputs:\n",
    "        - lemas (list):         Lista de los lemas de las palabras alfabéticas que se encuentran en el texto.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    lemas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    return lemas\n",
    "~~~\n",
    "Con esta función se lemmatiza cada texto existente en la columna *DESCRIPTION*.\n",
    "\n",
    "~~~\n",
    "def aplicar_funcion_a_columna(df, Dict_pickle, nombre_listas, columna=\"DESCRIPTION\"):\n",
    "    '''\n",
    "    Función que genera columnas en un dataframe en función de la presencia de determinados lemas en una columna específica de un Dataframe.\n",
    "\n",
    "    Por defecto, la columna es DESCRIPTION.\n",
    "\n",
    "    - Inputs: \n",
    "        - df (Dataframe):       Dataframe a tratar.\n",
    "        - Dict_pickle (dict):   Diccionario con listas\n",
    "        - nombre_listas (list): Lista proveniente del Diccionario Dict.\n",
    "        - columna (str):        Columna del dataframe a tratar.\n",
    "    \n",
    "    - Outputs: \n",
    "        - df (Dataframe):       Dataframe tratado.\n",
    "    '''\n",
    "\n",
    "    # Carga la lista desde el diccionario\n",
    "    lista = Dict_pickle[nombre_listas]\n",
    "\n",
    "    # Aplica la función a la columna del DataFrame\n",
    "    df[nombre_listas] = df[columna].apply(lambda x: any(lematizado in lista for lematizado in extraer_lemas(x)))\n",
    "\n",
    "    return df\n",
    "~~~\n",
    "Con esta función se buscan los diferentes valores de cada par clave-valor del diccionario en el texto lemmatizado de *DESCRIPTION* y crea una nueva columna cuyo nombre es el valor de la clave y cuyo valor en cada registro es True/False en el caso que se encuentre alguno o ninguno de los posibles valores que están incluidos en la lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generación de base de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la información que hemos obtenido y creado en los puntos anteriores vamos a proceder a guardarla en una base de datos para, posteriormente, poder aplicarla en otros procesos.\n",
    "\n",
    "Me he decidido en utilizar una base de datos utilizando *SQLite*, tras consultar con expertos en Full-Stack y orientarme sobre la operatibilidad de la misma al tener guardarla en un repositorio de GitHub.\n",
    "\n",
    "La estructura de la base de datos es:\n",
    "\n",
    "![Estructura_tablas](./Resources/image/Relacion_tablas.jpg)\n",
    "\n",
    "A partir de aquí generaremos las diferentes *tablas* y *views* que consideremos oportunas para el manejo de datos.\n",
    "\n",
    "Para los que no conozcan la importancia del uso de *views* en bases de datos, les recomiendo que miren en este [enlace](https://www.linkedin.com/feed/update/urn:li:activity:7058051069079093248?utm_source=share&utm_medium=member_desktop) donde hablo sobre el uso de views.  \n",
    "\n",
    "#### Estas serán las librerias necesarias para esta parte del proyecto:\n",
    "\n",
    "~~~~\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from Utils import functions as f\n",
    "~~~~\n",
    "\n",
    "También utilizaremos una función que tenemos guardada en Utils\\functions.py, que utilizamos para generar Pandas.Dataframe a partir de SQL.\n",
    "\n",
    "~~~\n",
    "def sql_query(query,cursor):\n",
    "    '''\n",
    "    Función que genera un dataframe a partir de una orden a la base de datos.\n",
    "\n",
    "    - Inputs:\n",
    "        - query (str):          Query realizada a la base de datos.\n",
    "        - cursor (cursor):      Cursor de conexión a la base de datos.\n",
    "\n",
    "    - Outputs:\n",
    "        - Dataframe (Dataframe): Dataframe de la query requerida.\n",
    "    '''\n",
    "\n",
    "    # Ejecuta la query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Almacena los datos de la query \n",
    "    ans = cursor.fetchall()\n",
    "\n",
    "    # Obtenemos los nombres de las columnas de la tabla\n",
    "    names = [description[0] for description in cursor.description]\n",
    "\n",
    "    return pd.DataFrame(ans,columns=names)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generación de BBDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos la Base de Datos en la carpeta de Resources desde la que podremos acceder en cualquier momento.\n",
    "\n",
    "Directamente  podemos crearla generando la conexión a la misma, el nombre de la misma será **online_shop.db**\n",
    "~~~\n",
    "conn = sqlite3.connect(\"Resources/online_shop.db\")\n",
    "cursor = conn.cursor()\n",
    "~~~\n",
    "\n",
    "#### Creamos las tablas a continuación:\n",
    "\n",
    "~~~\n",
    "query_product = '''\n",
    "CREATE TABLE PRODUCT  (\n",
    "    ID INT (3),\n",
    "    PRODUCT VARCHAR (100),\n",
    "    SLOGAN VARCHAR (100),\n",
    "    DESCRIPTION VARCHAR (250),\n",
    "    CHARACTERISTICS VARCHAR (250),\n",
    "    URL VARCHAR (50),\n",
    "    PRIMARY KEY (ID)\n",
    ")\n",
    "'''\n",
    "\n",
    "query_prices = '''\n",
    "CREATE TABLE PRICES  (\n",
    "    ID INT (3),\n",
    "    ID_PRODUCT INT (3),\n",
    "    REGULAR_PRICE INT (3),\n",
    "    DISCOUNT_PRICE INT (3),\n",
    "    DATE_DOWNLOAD VARCHAR (100),\n",
    "    PRIMARY KEY (ID),\n",
    "    FOREIGN KEY (ID_PRODUCT) REFERENCES PRODUCT (ID)\n",
    ")\n",
    "'''\n",
    "\n",
    "query_comment = '''\n",
    "CREATE TABLE COMMENT  (\n",
    "    ID_COMMENT INT (5),\n",
    "    ID_PRODUCT INT (3),\n",
    "    DATE VARCHAR (100),\n",
    "    RATIO INT (2),\n",
    "    ID_USERS INT (3),\n",
    "    COMMENT VARCHAR (250), \n",
    "    PRIMARY KEY (ID_COMMENT),\n",
    "    FOREIGN KEY (ID_PRODUCT) REFERENCES PRODUCT (ID),\n",
    "    FOREIGN KEY (ID_USERS) REFERENCES USERS (ID)\n",
    ")\n",
    "'''\n",
    "\n",
    "query_tags = '''\n",
    "CREATE TABLE TAGS  (\n",
    "    ID INT (3),\n",
    "    JUGUETES INT(1),\n",
    "    MUEBLES INT(1),\n",
    "    LENCERIA INT(1),\n",
    "    MASCULINO INT(1),\n",
    "    FEMININO INT(1),\n",
    "    ANAL INT(1),\n",
    "    BDSM INT(1),\n",
    "    AMENITIES INT(1),\n",
    "    PRIMARY KEY (ID),\n",
    "    FOREIGN KEY (ID) REFERENCES PRODUCT (ID)\n",
    ")\n",
    "'''\n",
    "\n",
    "query_users = '''\n",
    "CREATE TABLE USERS  (\n",
    "    ID INT (5),\n",
    "    USERS VARCHAR (50),\n",
    "    PRIMARY KEY (ID)\n",
    ")\n",
    "'''\n",
    "\n",
    "'''Ejecutamos las queries para generar las tablas'''\n",
    "\n",
    "cursor.execute(query_product)\n",
    "cursor.execute(query_prices)\n",
    "cursor.execute(query_comment)\n",
    "cursor.execute(query_tags)\n",
    "cursor.execute(query_users)\n",
    "~~~\n",
    "\n",
    "#### Creamos las vistas:\n",
    "\n",
    "~~~\n",
    "view_1='''\n",
    "CREATE VIEW COMMENTS_PER_PRODUCT\n",
    "SELECT PRODUCT.PRODUCT,COMMENT.DATE,COMMENT.RATIO ,COMMENT.COMMENT \n",
    "FROM COMMENT \n",
    "JOIN PRODUCT ON PRODUCT.ID=COMMENT.ID_PRODUCT\n",
    "'''\n",
    "\n",
    "view_2='''\n",
    "CREATE VIEW NUMBER_TAGS_PER_PRODUCT \n",
    "SELECT PRODUCT.PRODUCT, PRODUCT.ID,\n",
    "SUM(CASE WHEN TAGS.JUGUETES = TRUE THEN 1 ELSE 0 END),\n",
    "COMMENT.RATIO ,COMMENT.COMMENT \n",
    "FROM COMMENT \n",
    "JOIN PRODUCT ON PRODUCT.ID=COMMENT.ID_PRODUCT\n",
    "'''\n",
    "\n",
    "'''Ejecutamos las queries para generar las vistas'''\n",
    "\n",
    "cursor.execute(view_1)\n",
    "cursor.execute(view_2)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Carga de datos en la Base de datos,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los 5 dataframes generados, cargaremos directamente los datos de modo masivo en cada tabla.\n",
    "\n",
    "El proceso es general para cada tabla, modificando el número de campos de la misma.\n",
    "\n",
    "Pondremos como ejemplo el caso de la tabla *PRODUCT*:\n",
    "\n",
    "~~~\n",
    "lista_products= df_products.values.tolist()                                                         #   Convertimos todo el dataframe en una lista\n",
    "\n",
    "cursor.executemany(\"INSERT INTO PRODUCT VALUES (?,?,?,?,?,?)\", lista_products)                      #   Salvamos directamente todos los datos en la tabla PRODUCTS\n",
    "\n",
    "                                                                                                    #   Habrá tantos interrogantes como campos tenga la tabla donde se guarda.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cerramos la Base de Datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejamos la base de datos cerrada a la espera de tener que acceder a ella, bien para realizar actualizaciones en la misma con nuevos ficheros, bien para realizar llamamientos para visualización o carga de Dashboards.\n",
    "\n",
    "~~~\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A partir de los datos guardados en la base de datos, vamos a realizar un visionado de los diferentes puntos que podemos evaluar.\n",
    "\n",
    "Veremos que información disponemos en relación con los productos y los usuarios.\n",
    "\n",
    "Todo el proceso de tratamiento de la información que disponemos la realizaremos a partir de comandos SQL que pasaremos a un dataframe para realizar el visionado.\n",
    "\n",
    "#### Estas serán las librerias necesarias para esta parte del proyecto:\n",
    "\n",
    "~~~~\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "~~~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 PRICE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En relación con este punto, vamos a realizar diversos estudios relacionados con los precios que tienen los productos y los descuentos que se aplican.\n",
    "\n",
    "Sobre esta tabla realizaremos visionados referentes a los precios más altos, con y sin descuentos realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 APPLIED DISCOUNT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué porcentaje de productos tienen un descuento aplicado?**\n",
    "\n",
    "Creo que esta pregunta es muy relevante dado que al visitar la página web te encuentras con muchos productos con descuento.\n",
    "\n",
    "Es por ello que vamos a ver qué cantidad de productos tienen y qué cantidad de productos no tienen descuento.\n",
    "\n",
    "Para ello vamos a generar un nuevo parámetro que será *PERCENTUAL DISCOUNT* cuyo valor será: ([DISCOUNT_PRICE]-[REGULAR_PRICE])/[REGULAR_PRICE] y que compararemos con el *PRICE* del producto.\n",
    "\n",
    "Al tener solo 2 posibles valores vamos a visualizar esto con un *Donut chart*.\n",
    "\n",
    "~~~\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"PERCENTAGE OF OFFERS\", fontsize=16)\n",
    "\n",
    "label=['WITH_DISCOUNT','WITHOUT_DISCOUNT']                                  \n",
    "\n",
    "'''Definimos un pieplot en blanco para dejar solo una circunferencia de la estrechez indicada'''\n",
    "e=0.7\n",
    "my_circle=plt.Circle( (0,0), e, color='white')            \n",
    "\n",
    "query='''\n",
    "SELECT PRODUCT.PRODUCT AS NAME, CAST(DATE_DOWNLOAD as INT) as DATE, REGULAR_PRICE,\n",
    "[DISCOUNT_PRICE]-[REGULAR_PRICE] AS DISCOUNT,\n",
    "ABS([DISCOUNT_PRICE]-[REGULAR_PRICE])/[REGULAR_PRICE]*100 as PERCENTUAL_DISCOUNT,           \n",
    "CASE WHEN [DISCOUNT_PRICE] = [REGULAR_PRICE] THEN 0 ELSE 1 END AS WITH_DISCOUNT\n",
    "FROM PRICES  \n",
    "JOIN PRODUCT ON PRODUCT.ID=PRICES.ID_PRODUCT\n",
    "ORDER BY DISCOUNT DESC\n",
    "'''                                                        \n",
    "discount_df=f.sql_query(query,cursor)\n",
    "\n",
    "data = discount_df.groupby(['DATE'])[\"WITH_DISCOUNT\"].value_counts()\n",
    "\n",
    "plt.pie(data.values,\n",
    "        labels=label,\n",
    "        autopct='%1.2f%%',\n",
    "        textprops={'fontsize': 8},\n",
    "        startangle=90, counterclock=True)\n",
    "\n",
    "plt.ylabel(data.index[0][0],fontsize=12)\n",
    "plt.yticks()\n",
    "plt.xticks(rotation=90)\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "~~~\n",
    "\n",
    "![Relacion_descuento](./Resources/Graphics/Percentage_offers.png)\n",
    "\n",
    "Como vemos en el grafico solo un 0,5% de los productos no tienen descuentos, lo que quiere decir que todo el inventario que se vende tiene un descuento aplicado.\n",
    "\n",
    "Si está situación es habitual cabría la posibilidad de plantearse que sigue una política continua de *descuento permanente* en el producto y que el \"precio sin descuento\" no se aplica nunca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué porcentaje de descuento tienen los productos?**\n",
    "\n",
    "Hemos visto que prácticamente todos los productos tienen descuentos, pero ¿cómo está distribuido el porcentaje aplicado en los productos?\n",
    "\n",
    "Es lo que vamos a ver a continuación.\n",
    "\n",
    "Para ello vamos a realizar un *boxplot* para ver las distancias intercuantílicas, por un lado, y un *swarmplot* para ver cuantitativamente la distribución de los precios.\n",
    "\n",
    "~~~\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"DISCOUNT DISTRIBUTION\")\n",
    "plt.ylabel(\"DISCOUNT %\")\n",
    "plt.xlabel(\"\")\n",
    "\n",
    "query='''\n",
    "SELECT PRODUCT.PRODUCT AS NAME, CAST(DATE_DOWNLOAD as INT) as DATE, REGULAR_PRICE, DISCOUNT_PRICE,\n",
    "[DISCOUNT_PRICE]-[REGULAR_PRICE] AS DISCOUNT,\n",
    "CAST(ABS([DISCOUNT_PRICE] - [REGULAR_PRICE]) / [REGULAR_PRICE] * 100 AS INT) AS PERCENTUAL_DISCOUNT\n",
    "FROM PRICES \n",
    "JOIN PRODUCT ON PRODUCT.ID=PRICES.ID_PRODUCT\n",
    "ORDER BY PERCENTUAL_DISCOUNT DESC\n",
    "'''                                           \n",
    "\n",
    "sns.boxplot(data=f.sql_query(query,cursor)['PERCENTUAL_DISCOUNT'],color=\"pink\")\n",
    "sns.swarmplot(f.sql_query(query,cursor)['PERCENTUAL_DISCOUNT'],color='black')\n",
    "\n",
    "'''Definiendo el outlier inferior'''\n",
    "outlier_1 = np.percentile(f.sql_query(query,cursor)['PERCENTUAL_DISCOUNT'], 1.1)        \n",
    "plt.axhline(y=outlier_1, xmin=0, xmax=1,color='r', linewidth= 1.5,linestyle=\"-.\")\n",
    "~~~\n",
    "\n",
    "![Relacion_descuento](./Resources/Graphics/discount_distribution.png)\n",
    "\n",
    "El umbral inferior de los valores por debajo de 1,1% sean outliers.\n",
    "\n",
    "De hecho es algo que visualmente se ve, en general los productos están por encima de 20%. Muy distribuidos entre el 20% y el 80% de descuento.\n",
    "\n",
    "Se puede ver claramente que el número de productos que serían outliers serían 7, siendo algunos de estos productos los que no tienen descuento.\n",
    "\n",
    "En general y por termino medio los productos tienen aproximadamente un descuento que ronda la mitad de su precio. Es por ello la siguiente cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 PRICE DISTRIBUTION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo se distribuyen los precios de los productos?**\n",
    "\n",
    "Vamos a realizar un estudio tanto del precio rebajado como el precio sin rebajar, queremos ver como se comportan los precios en sí mismos para sacar conclusiones.\n",
    "\n",
    "Para ello vamos a realizar dos subplots con un *histograma* en cada uno.\n",
    "\n",
    "Como bien podemos imaginar hay una gran variedad de precios, por ello lo primero que realizaremos es una modificación en los precios para poder agrupar mejor los valores.\n",
    "\n",
    "Para ello vamos a generar un nuevo parámetro que será *ABSOLUTE_PRICE* cuyo valor será: CAST(DISCOUNT_PRICE/10 AS INT)*10.\n",
    "\n",
    "Dejaremos los valores en decenas de unidad para aguparlos.\n",
    "\n",
    "~~~\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5), sharey=True)\n",
    "axes[0].set_position([0,1.5,0.5,0.300])\n",
    "axes[1].set_position([0.5,1.5,0.2,0.300])\n",
    "\n",
    "label=[\"Discount Price Distribution\",\"Regular Price Distribution\"]\n",
    "\n",
    "'''PLOT 0'''\n",
    "\n",
    "query_1='''\n",
    "SELECT PRODUCT.PRODUCT AS NAME,CAST(DISCOUNT_PRICE/10 AS INT)*10 as ABSOLUTE_PRICE\n",
    "FROM PRICES \n",
    "JOIN PRODUCT ON PRODUCT.ID=PRICES.ID_PRODUCT\n",
    "'''                                \n",
    "abs_disc_price_df=f.sql_query(query_1,cursor)\n",
    "sns.distplot(abs_disc_price_df['ABSOLUTE_PRICE'],bins=50,color=\"teal\", ax=axes[0])\n",
    "\n",
    "\n",
    "'''PLOT 1'''\n",
    "\n",
    "query_2='''\n",
    "SELECT PRODUCT.PRODUCT AS NAME, CAST(REGULAR_PRICE/10 AS INT)*10 as ABSOLUTE_PRICE\n",
    "FROM PRICES \n",
    "JOIN PRODUCT ON PRODUCT.ID=PRICES.ID_PRODUCT\n",
    "'''                                \n",
    "abs_reg_price_df=f.sql_query(query_2,cursor)\n",
    "sns.distplot(abs_reg_price_df['ABSOLUTE_PRICE'],bins=50,color=\"olive\", ax=axes[1])\n",
    "\n",
    "for i in range (0,2):\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].set_xlabel('PRICE')\n",
    "    axes[i].set_title(label[i])\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), fontsize=10)\n",
    "\n",
    "fig.suptitle(\"Prices_Distribution_study\",fontsize=15)\n",
    "fig.tight_layout() \n",
    "~~~\n",
    "\n",
    "![Histograma_precios](./Resources/Graphics/Prices_Distribution_study.png)\n",
    "\n",
    "En relación con el **primer subplot**, que hace referencia al precio rebajado, hay una gran variedad de densidad de productos que oscilan entre los precios de 0-10 € hasta los productos que llevan a los 100 €, aunque principalmente los precios se agrupan más en torno entre los 0 € y los 30 €.\n",
    "\n",
    "Además los precios rebajados no superan los 350 € como vemos al final de su histograma.\n",
    "\n",
    "En relación con el **segundo subplot**, que hace referencia al precio sin rebajar, el rango de precios es mayor, llegando hasta 170 € y con una densidad más repartida.\n",
    "\n",
    "Además los precios pueden llegar hasta los 1000 € como vemos al final de su histograma."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 COMMENTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que tenemos de comentarios cubre un amplio abanico en el **tiempo**, es por ello que podemos hacer un estudio sobre la evolución en el tiempo de este campo y ver cuando se realizan más comentarios y cuando menos.\n",
    "\n",
    "Además los comentarios van acompañados de una **evaluación** de los productos comprados por lo que podemos hacer un estudio de este aspecto.\n",
    "\n",
    "Finalmente también podemos hacer uso de procesos de NLP para hacer una evaluación del **sentimiento** de los usuarios sobre los productos comentados y ver el grado de satisfacción con los mismos.\n",
    "\n",
    "Procedemos pues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 TIME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que tenemos un campo de fechas en la realización de los comentarios, vamos a proceder a ver cuando se realizan los usuarios comentarios sobre los productos comprados.\n",
    "\n",
    "Disponemos de datos sobre los años, los meses, los días del mes, los días de las semana en que se realizan estos comentarios, por lo que podemos ver pautas de comportamiento.\n",
    "\n",
    "En este punto, tenemos que realizar la suposición que el comentario se realiza en el momento de la compra del producto y que todos los productos que se compran son comentados automáticamente por los usuarios, lo cual no deja de no ser cierto.\n",
    "\n",
    "Sería interesante en este sentido disponer de la información real de compras de estos productos para conocer por ejemplo qué ratio de conversión de compra a comentario existe y ver si la página web tiene una comunidad viva que aporte.\n",
    "\n",
    "También hay que tener en cuenta que existen una serie de productos que tienen compras repetitivas en el tiempo, pero que por este motivo es de pensar que solo tendrán un único comentario por usuario que realiza la compra, serían productos que hemos asignado como *AMENITIES*.\n",
    "\n",
    "El análisis que vamos a realizar solo contemplará la información de tiempo de *AÑO*, *MES* y *DIA DE LA SEMANA*.\n",
    "\n",
    "Como el proceso tratamiento del campo *DATE* es el mismo y solo se realiza una agrupación de la fecha, vamos a mostrar la información en 3 subplots de barras.\n",
    "\n",
    "~~~\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5), sharey=True)\n",
    "axes[0].set_position([0,1.5,0.5,0.300])\n",
    "axes[1].set_position([0.5,1.5,0.2,0.300])\n",
    "axes[2].set_position([1.5,1.5,0.2,0.300])\n",
    "\n",
    "label=[\"YEAR\",\"MONTH\",\"DAY OF THE WEEK\"]\n",
    "\n",
    "'''PLOT 0'''\n",
    "\n",
    "query='''\n",
    "select count (COMMENT) AS COMENTARIOS, strftime ('%Y',DATE) as YEAR \n",
    "from COMMENT \n",
    "GROUP BY YEAR\n",
    "'''\n",
    "total=f.sql_query(query,cursor)\n",
    "total.loc[len(total)] = [0,2006]\n",
    "total.iloc[:,1]=total.iloc[:,1].astype(int)\n",
    "sns.barplot(x=total['YEAR'],y=total['COMENTARIOS'],color=\"skyblue\", ax=axes[0])\n",
    "\n",
    "'''PLOT 1'''\n",
    "\n",
    "query='''\n",
    "select count (COMMENT) AS COMENTARIOS, strftime ('%m',DATE) as MONTH \n",
    "from COMMENT \n",
    "GROUP BY MONTH\n",
    "'''   \n",
    "total=f.sql_query(query,cursor)\n",
    "total.iloc[:,1]=total.iloc[:,1].astype(int)\n",
    "sns.barplot(x=total['MONTH'],y=total['COMENTARIOS'],color=\"steelblue\", ax=axes[1])\n",
    "\n",
    "'''PLOT 2'''\n",
    "\n",
    "query='''select count (COMMENT) AS COMENTARIOS, strftime ('%w',DATE) as DAY_WEEK \n",
    "from COMMENT \n",
    "GROUP BY DAY_WEEK\n",
    "'''   \n",
    "total=f.sql_query(query,cursor)\n",
    "total.iloc[:,1]=total.iloc[:,1].astype(int)\n",
    "total['DAY_WEEK']=total['DAY_WEEK'].map(dicc_dia)\n",
    "sns.barplot(x=total['DAY_WEEK'],y=total['COMENTARIOS'],color=\"teal\", ax=axes[2])\n",
    "\n",
    "for i in range (0,3):\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_title(label[i])\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, fontsize=7)\n",
    "\n",
    "fig.suptitle(\"COMMENTS BY DATE OF STUDY\",fontsize=15)\n",
    "fig.tight_layout() \n",
    "~~~\n",
    "![Histograma_comentarios](./Resources/Graphics/Comments_per_date.png)\n",
    "\n",
    "En este punto podemos realizar varias **conclusiones**:\n",
    "\n",
    "1) En relación con el **año**:\n",
    "\n",
    "    Hay un crecimiento de comentarios hasta aproximademente el 2015, para después estancarse. También se observa un nuevo pico en el 2020, puede ser que sea por el COVID-19, pero el resto de los años hay una constante de comentarios en torno a los 900 comentarios.\n",
    "\n",
    "    Los datos que se muestran de 2023 son más bajos, dado que la extracción de la información no es completa de ese año.\n",
    "\n",
    "2) En relación con los **meses**:\n",
    "\n",
    "    Se observa mucha estabilidad en los datos, habiendo picos de comentarios en enero (coincidiendo con Reyes) y en Septiembre (coincidiendo con la vuelta en la mayoría de los casos de las vacaciones).\n",
    "\n",
    "    El tema de las vacaciones puede ser algo interesante de evaluar en el sentido de si los usuarios tienen pareja con niños o si realizan sus compras durante los meses estivales anteriores y comentan a la vuelta y desvirtua en parte este análisis.\n",
    "\n",
    "3) En relación con el **día de la semana**:\n",
    "\n",
    "    Se observa que los días de la semana con menores comentarios sean los días del fin de semana y en general se agrupan en los días entre semana.\n",
    "\n",
    "    Esto puede ser porque al comercializar un producto de ocio, esté se compre y se comente fuera del fin de semana, que es cuando más podría ser utilizado al no ser productos de consumo inmediato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 RATINGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto como se distribuye la presencia de comentarios en el tiempo, pero **¿cómo son vistos los productos por los usuarios?**\n",
    "\n",
    "Esta es la pregunta que nos hacemos ahora.\n",
    "\n",
    "Tenemos una fuente directa de esta informacion en la página web, las estrellas que les dan los usuarios que van desde 1 a 5.\n",
    "\n",
    "También podríamos usar una fuente indirecta, la perdida de usuarios por la página web, pero esto es algo que no vamos a entrar en este proyecto.\n",
    "\n",
    "Vamos a realizar un *histograma* de valoraciones para ver el promedio de valoración de cada producto.\n",
    "~~~\n",
    "plt.figure(figsize=(10,5))\n",
    "query='''\n",
    "SELECT PRODUCT,avg(RATIO) as PROM \n",
    "FROM COMMENTS_PER_PRODUCT \n",
    "group BY PRODUCT \n",
    "ORDER BY avg(RATIO) DESC\n",
    "'''\n",
    "total=f.sql_query(query,cursor)\n",
    "sns.histplot(data=total,bins=100)\n",
    "plt.xticks()\n",
    "plt.ylabel(\"Nº PRODUCTS\")\n",
    "plt.xlabel(\"RATING\")\n",
    "plt.title(\"DISTRIBUTION OF PRODUCT RATIOS\")\n",
    "file=str(r'\\Resources/Graphics/ratios_per_product')\n",
    "plt.legend(labels=[])\n",
    "~~~\n",
    "![Distribucion_comentarios](./Resources/Graphics/ratios_per_product.png)\n",
    "\n",
    "Podemos ver que prácticamente todos los productos tienen una valoración superior a 4, con una alta cantidad de productos con una valoración de 5 estrellas.\n",
    "\n",
    "¿Cabe la posibilidad que entonces los productos inferiores a 4 estrellas sean outliers?¿y los de 5 estrellas?\n",
    "\n",
    "Esto es lo que vamos a ver a continuación.\n",
    "\n",
    "Para ello vamos a realizar un *boxplot* para evaluar los cuartiles existentes.\n",
    "\n",
    "~~~\n",
    "plt.figure(figsize=(10,5))\n",
    "query='''\n",
    "SELECT PRODUCT,avg(RATIO) as PROM \n",
    "FROM COMMENTS_PER_PRODUCT \n",
    "group BY PRODUCT\n",
    "ORDER BY avg(RATIO) DESC\n",
    "'''\n",
    "sns.boxplot(data=f.sql_query(query,cursor)['PROM'])\n",
    "outlier = np.percentile(f.sql_query(query,cursor)['PROM'], 5.5)\n",
    "plt.axhline(y=outlier, xmin=0, xmax=1,color='r', linewidth= 1,linestyle=\"-.\")\n",
    "plt.ylabel(\"RATING\")\n",
    "plt.title(\"DISTRIBUTION OF PRODUCT RATIOS\")\n",
    "~~~\n",
    "![Distribucion_comentarios](./Resources/Graphics/Outliers_per_product.png)\n",
    "\n",
    "Podemos ver claramente que los productos con una valoración inferior a 3.75 estrellas aproximadamente se podrían considerar outliers.\n",
    "\n",
    "También que los productos con una media de valoración de 5 estrellas no son outliers.\n",
    "\n",
    "Los datos obtenidos en estudios de estos datos indican que por debajo de 3,75 estrellas son 23 productos y mientras que las que tienen 5 estrellas son 129, lo que representan un 5.15 % y 28.86 % de los productos del inventario de la página web.\n",
    "\n",
    "Que más de un 25 % de los productos tengan la valoración máxima implica que, por lo general, todos los usuarios que hacen comentarios están contentos con los productos que compran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 SENTIMENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este punto es un aspecto que dada la información que hemos obtenido en el apartado anterior, no vamos a valorar en este proyecto.\n",
    "\n",
    "Como hemos visto la mediana se encuentra en 4.75 puntos y hay mucha acumulación de comentarios con la máxima puntuación por lo que el sentimiento de los comentarios va a ser bueno.\n",
    "\n",
    "Dejamos para un posible futuro la posibilidad de realizar este estudio pero en los comentarios de menor valoración."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 NUMBER OF USERS IN THE COMMUNITY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este punto puede ser crítico.\n",
    "\n",
    "**¿Cuántos usuarios han realizado un comentario al menos en el intervalo de tiempo que lleva existiendo la página web?**\n",
    "\n",
    "Lo podemos ver directamente con esta consulta:\n",
    "\n",
    "~~~\n",
    "query='''\n",
    "SELECT COUNT(COMMENT)AS nº_COMMENTS, USERS.USERS AS USERS\n",
    "FROM COMMENT \n",
    "JOIN USERS\n",
    "ON COMMENT.ID_USERS=USERS.ID\n",
    "GROUP BY USERS\n",
    "ORDER BY nº_COMMENTS DESC\n",
    "'''\n",
    "users=f.sql_query(query,cursor)\n",
    "print(\"El número de usuarios que han comentado en algún momento un producto es\", len(users))\n",
    "~~~\n",
    "\n",
    "La respuesta a esta pregunta son 2021 usuarios. 2021 usuarios que han realizado un número indefinido de comentarios.\n",
    "\n",
    "Pero, **¿cuántos de estos usuarios han realizado únicamente 1 comentario?**\n",
    "\n",
    "~~~\n",
    "user_comment_once=users[users['nº_COMMENTS']==1]\n",
    "print(\"El número de usuarios que han comentado unicamente una vez es\",len(user_comment_once))\n",
    "~~~\n",
    "\n",
    "La respuesta a esta pregunta es 827 usuarios. \n",
    "\n",
    "Esto representa casi la mitad de los usuarios que han realizado un comentario, por lo que se puede decir que o bien los usuarios solo realizan un único comentario o deja de comprar en esta página web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 DISTRIBUTION OF USERS BY YEAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo se comportan los usuarios en el tiempo?**\n",
    "\n",
    "Para proseguir con este estudio vamos a incorporar a este análisis univariante una segunda variable, el año de realización del comentario.\n",
    "\n",
    "Vamos a ver la evolución de los usuarios en el tiempo y ver si los usuarios se mantienen en el tiempo.\n",
    "\n",
    "~~~\n",
    "query='''\n",
    "SELECT COUNT(COMMENT)AS nº_COMMENTS, strftime ('%Y',DATE) as YEAR, USERS.USERS AS USERS\n",
    "FROM COMMENT \n",
    "JOIN USERS\n",
    "ON COMMENT.ID_USERS=USERS.ID\n",
    "GROUP BY USERS,YEAR\n",
    "ORDER BY YEAR asc\n",
    "'''\n",
    "\n",
    "comments_users_per_year=f.sql_query(query,cursor)\n",
    "comments_users_per_year = comments_users_per_year.pivot(index='USERS', columns='YEAR', values='nº_COMMENTS')\n",
    "comments_users_per_year = comments_users_per_year.fillna(0)\n",
    "comments_users_per_year['TOTAL_COMMENTS'] = comments_users_per_year.sum(axis=1)                                             \n",
    "comments_users_per_year = comments_users_per_year.sort_values(by='TOTAL_COMMENTS', ascending=False)                         \n",
    "comments_users_per_year\n",
    "~~~\n",
    "\n",
    "y vamos a considerar el TOP 50 de usuarios para poder realizar un estudio *sólo* sobre aquellos usuarios que son activos en la comunidad y ver la información en un *heatmap*.\n",
    "\n",
    "![heatmap_usuarios](./Resources/Graphics/N_TOP_Commented_Users_by_YEAR.png)\n",
    "\n",
    "Aqui vemos 2 situaciones diferentes:\n",
    "1) Hay casos puntuales donde un usuario aparece fugazmente, realiza una gran cantidad de comentarios y luego desaparece.\n",
    "2) Hay casos que realizan unos pocos comentarios cada año, pero son fieles a la comunidad y aportan a ella.\n",
    "\n",
    "En cualquier caso, no hay que olvidar que el mayor problema para este análisis es que hemos generado la variable *USERS* a partir de los nombres de los comentarios y, por lo tanto, si dos usuarios distintos tienen el mismo nombre y comentan dos productos distintos, este proceso los consideran como si fueran el mismo usuario.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 TAGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas o TAGS de los productos son algo inherente a cada producto, es un modo de poder asignar características comunes a los mismos y poder clasificar así los mismos.\n",
    "\n",
    "Lo bueno que tiene esto es que podemos clasificar a los usuarios en relación a su comportamiento de compra de los mismos.\n",
    "\n",
    "Aquí vamos a considerar 2 preguntas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 TAGS BY PRODUCT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué tipo de productos tenemos en nuestro inventario?**\n",
    "\n",
    "Es una pregunta fácil, pero con esto podemos saber qué tipo de productos son más demandados y cuales no.\n",
    "\n",
    "Para evaluar esta pregunta vamos a realizar un *lollipop* con cada uno de los posibles TAGS que existen.\n",
    "\n",
    "Como cada TAG es una columna independiente, crearemos una variable que tenga en cuenta el valor total de los TAGS. \n",
    "\n",
    "~~~\n",
    "plt.figure(figsize=(10,5))\n",
    "query = '''\n",
    "SELECT 'JUGUETES' AS TAGS, SUM(JUGUETES) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'MUEBLES' AS TAGS, SUM(MUEBLES) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'LENCERIA' AS TAGS, SUM(LENCERIA) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'MASCULINO' AS TAGS, SUM(MASCULINO) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'FEMININO' AS TAGS, SUM(FEMININO) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'ANAL' AS TAGS, SUM(ANAL) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'BDSM' AS TAGS, SUM(BDSM) AS valor FROM TAGS\n",
    "    UNION\n",
    "    SELECT 'AMENITIES' AS TAGS, SUM(AMENITIES) AS valor FROM TAGS\n",
    "ORDER  BY valor DESC;\n",
    "'''\n",
    "\n",
    "total=f.sql_query(query,cursor)\n",
    "total.iloc[:,1]=total.iloc[:,1].astype(int)\n",
    "plt.hlines(total['TAGS'],xmin=0,\n",
    "           xmax=total['valor'],)\n",
    "plt.plot(total['valor'], total['TAGS'], \"o\")\n",
    "plt.yticks(total['TAGS'])\n",
    "plt.xlabel(\"Nº PRODUCT\")\n",
    "plt.ylabel(\"TAGS\")\n",
    "plt.title(\"Nº PRODUCT PER TAG\")\n",
    "~~~\n",
    "![tags_product](./Resources/Graphics/N_Tags_product.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 TAGS BY USERS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué tipo de productos buscan los usuarios?**\n",
    "\n",
    "Pretendemos tener una visión sobre los tipos de productos que compran y valoran los usuarios.\n",
    "\n",
    "Del mismo modo que en el punto anterior, procederemos a relacionar los *TAGS* con los *USERS* en la consulta.\n",
    "\n",
    "![tags_product](./Resources/Graphics/N_Tags_users.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No sabemos si tiene relación, pero si comparamos las dos graficas frente a frente, podemos observar que el comportamiento de las TAGS son similares.\n",
    "\n",
    "![tags_product](./Resources/Graphics/TAGS_study.png)\n",
    "\n",
    "Lo que podemos decir en relación con estos datos que vemos es que no hay muchos usuarios que comenten productos dentro de *AMENITIES*, bien porque son pocos productos los que llevan esta etiqueta, bien porque son productos que como solo se comentan una vez por usuario (no es que no lo permita la página web, pero nadie comenta varias veces un mismo producto).\n",
    "\n",
    "También vemos que hay muchos productos y muchos usuarios que suelen comentar productos con la etiqueta *ANAL*, teniendo además en cuenta que al generar este tag, se usa una lista de las más pequeñas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 CONCLUSIONES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras esta evaluación de los datos recabados creo que podemos tener claro varios puntos:\n",
    "\n",
    "1) Por norma general los productos tienen descuento sobre el precio oficial, además esta rebaja oscila en torno al 50 %, es decir cuesta la mitad de lo que dicen que cuesta.\n",
    "\n",
    "    Los precios de esta manera suelen oscilar entre 0 € y 100 €.\n",
    "\n",
    "2) Los periodos en los cuales los productos son comentados (comprados) son los meses de enero y septiembre y preferiblemente a mediados de semana.\n",
    "\n",
    "    No hay ninguna modificación en cuanto al volumen a lo largo de los años, los comentarios se han quedado estancados en torno a los 900 comentarios/año.\n",
    "\n",
    "3) Los productos tienen una muy buena valoración por los usuarios que realizan los comentarios.\n",
    "\n",
    "    Prácticamente los productos tienen un 4 o un 5 de valoración y solo un 5% de los productos tienen baja valoración\n",
    "\n",
    "4) A pesar de contabilizar más de 2000 usuarios que han realizado al menos un comentario, más del 40% solo han realizado un único comentario.\n",
    "\n",
    "    Esto implica una baja actividad en la comunidad por parte de los usuarios, pero no sabemos si la baja actividad lleva consigo que hayan abandonado la comunidad o no.\n",
    "    \n",
    "    También podemos ver entre los 50 principales usuarios, que tenemos registrados en la base de datos, que se tratan de usuarios con actividad en la comunidad, ya que lo habitual es que comenten varios productos a lo largo de los años.\n",
    "\n",
    "5) En relación con las TAGS podemos ver la poca presencia de comentarios en *AMENITIES* lo cual es implicito al hecho que son productos de compra repetitiva, pero que no se comentan varias veces por el mismo usuario.\n",
    "\n",
    "    Hay una relación directa entre los productos comentados y los usuarios, una TAG con muchos productos lleva implicito muchos usuarios que lo comentan y viceversa.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afde6861a040563f15a2ec1b440faf84809f9a7bcc3c75cfd11a60e7dd448719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
